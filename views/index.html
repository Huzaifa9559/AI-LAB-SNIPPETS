<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>CSP Examples with OR-Tools</title>
  <style>
    body { font-family: Arial, sans-serif; padding: 20px; background: #f4f4f4; }
    h2 { margin-top: 40px; color: #333; }
    pre { background: #fff; padding: 12px; border-left: 4px solid #007acc; overflow-x: auto; }
    code { font-family: Consolas, monospace; font-size: 14px; white-space: pre-wrap; }
  </style>
</head>
<body>
  <h1>üß† Constraint Satisfaction Problem (CSP) Examples</h1>

  <h2>1. Basic CSP Template (Always Start With This)</h2>
  <pre><code>from ortools.sat.python import cp_model
  
  model = cp_model.CpModel()
  
  # Define variables
  x = model.new_int_var(0, 2, "x")
  y = model.new_int_var(0, 2, "y")
  
  # Define constraints
  model.add(x != y)
  
  # Solve
  solver = cp_model.CpSolver()
  status = solver.solve(model)
  if status in [cp_model.FEASIBLE, cp_model.OPTIMAL]:
      print(f"x = {solver.value(x)}, y = {solver.value(y)}")
  else:
      print("No solution found.")
  </code></pre>
  
  <h2>ü™ë Seating Arrangement Puzzle (Logic Constraints)</h2>
  <pre><code>from ortools.sat.python import cp_model
  
  model = cp_model.CpModel()
  
  Amir = model.NewIntVar(1, 4, "Amir")
  Bella = model.NewIntVar(0, 5, "Bella")
  Charles = model.NewIntVar(0, 5, "Charles")
  Diana = model.NewIntVar(1, 4, "Diana")
  Ethan = model.NewIntVar(2, 3, "Ethan")
  Farah = model.NewIntVar(0, 5, "Farah")
  
  panelists = [Amir, Bella, Charles, Diana, Ethan, Farah]
  names = ["Amir", "Bella", "Charles", "Diana", "Ethan", "Farah"]
  
  model.AddAllDifferent(panelists)
  model.Add(Bella < Farah)
  model.AddAbsEquality(model.NewIntVar(1, 1, "Charles_Diana_adjacent"), Charles - Diana)
  
  solver = cp_model.CpSolver()
  status = solver.Solve(model)
  
  if status == cp_model.FEASIBLE or status == cp_model.OPTIMAL:
      seat_map = {}
      for var, name in zip(panelists, names):
          seat_map[solver.Value(var)] = name
  
      print("Seat assignments (from left S0 to right S5):")
      for seat in range(6):
          print(f"Seat {seat}: {seat_map[seat]}")
  else:
      print("No solution found.")
  </code></pre>
  
  <h2>üìÜ 3. Schedule Tasks (Time Slots)</h2>
  <pre><code>from ortools.sat.python import cp_model
  
  slots = range(3)
  model = cp_model.CpModel()
  tasks = [model.new_int_var(0, 2, f"task_{i}") for i in slots]
  
  model.add_all_different(tasks)
  
  solver = cp_model.CpSolver()
  solver.solve(model)
  for i, t in enumerate(tasks):
      print(f"Task {i} ‚Üí Time Slot {solver.value(t)}")
  </code></pre>
  
  <h2>ü§ñ 4. Diagonal Robot Navigation (5x5 Grid)</h2>
  <pre><code>from ortools.sat.python import cp_model
  import math
  
  GRID_SIZE = 5
  obstacles = set()
  start = (1, 1)
  target = (4, 4)
  
  valid_moves = [(-1, -1), (-1, 0), (-1, 1),
                 (0, -1),           (0, 1),
                 (1, -1),  (1, 0),  (1, 1)]
  
  class DiagonalPathSolutionPrinter(cp_model.CpSolverSolutionCallback):
      def __init__(self, variables, grid_size):
          super().__init__()
          self.variables = variables
          self.grid_size = grid_size
          self.solution_count = 0
  
      def on_solution_callback(self):
          self.solution_count += 1
          path = []
          for i in range(len(self.variables)):
              x = self.Value(self.variables[i][0])
              y = self.Value(self.variables[i][1])
              path.append((x, y))
          print(f"Solution {self.solution_count}: {path}")
  
  def solve_diagonal_path():
      model = cp_model.CpModel()
      max_steps = 2 * GRID_SIZE
  
      steps = [(model.NewIntVar(0, GRID_SIZE - 1, f"x_{i}"),
                model.NewIntVar(0, GRID_SIZE - 1, f"y_{i}"))
               for i in range(max_steps)]
  
      model.Add(steps[0][0] == start[0])
      model.Add(steps[0][1] == start[1])
      model.Add(steps[-1][0] == target[0])
      model.Add(steps[-1][1] == target[1])
  
      for i in range(max_steps - 1):
          dx = model.NewIntVar(-1, 1, f"dx_{i}")
          dy = model.NewIntVar(-1, 1, f"dy_{i}")
          model.AddAllowedAssignments([dx, dy], valid_moves)
          model.Add(steps[i + 1][0] == steps[i][0] + dx)
          model.Add(steps[i + 1][1] == steps[i][1] + dy)
          for ox, oy in obstacles:
              model.AddForbiddenAssignments([steps[i + 1][0], steps[i + 1][1]], [(ox, oy)])
  
      cost = sum(math.sqrt(2) for _ in range(max_steps - 1))
      model.Minimize(int(cost * 100))
  
      solver = cp_model.CpSolver()
      solution_printer = DiagonalPathSolutionPrinter(steps, GRID_SIZE)
      solver.parameters.enumerate_all_solutions = True
      status = solver.Solve(model, solution_printer)
  
      if status in (cp_model.OPTIMAL, cp_model.FEASIBLE):
          print("Optimal path found!")
      else:
          print("No valid path found.")
  
  solve_diagonal_path()
  </code></pre>
  
  <h2>üåä Island Perimeter Detection (Largest Landmass via BFS + CSP)</h2>
  <pre><code>from ortools.sat.python import cp_model
  from collections import deque
  
  grid = [[0, 1, 1, 0], [1, 1, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0]]
  rows = len(grid)
  cols = len(grid[0])
  
  def get_largest_landmass(grid):
      visited = [[False] * cols for _ in range(rows)]
      max_component = []
  
      def bfs(r, c):
          queue = deque()
          component = []
          queue.append((r, c))
          visited[r][c] = True
  
          while queue:
              x, y = queue.popleft()
              component.append((x, y))
              for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
                  nx, ny = x + dx, y + dy
                  if 0 <= nx < rows and 0 <= ny < cols:
                      if not visited[nx][ny] and grid[nx][ny] == 1:
                          visited[nx][ny] = True
                          queue.append((nx, ny))
          return component
  
      for i in range(rows):
          for j in range(cols):
              if grid[i][j] == 1 and not visited[i][j]:
                  comp = bfs(i, j)
                  if len(comp) > len(max_component):
                      max_component = comp
      return max_component
  
  largest_landmass = get_largest_landmass(grid)
  
  model = cp_model.CpModel()
  edge_vars = []
  
  for r, c in largest_landmass:
      for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
          nr, nc = r + dr, c + dc
          is_edge = model.NewBoolVar(f"edge_{r}_{c}_{dr}_{dc}")
          if not (0 <= nr < rows and 0 <= nc < cols) or grid[nr][nc] == 0:
              model.Add(is_edge == 1)
          else:
              model.Add(is_edge == 0)
          edge_vars.append(is_edge)
  
  perimeter = model.NewIntVar(0, len(edge_vars), "perimeter")
  model.Add(perimeter == sum(edge_vars))
  model.Maximize(perimeter)
  
  solver = cp_model.CpSolver()
  status = solver.Solve(model)
  
  if status in (cp_model.OPTIMAL, cp_model.FEASIBLE):
      print("Perimeter of the largest landmass:", solver.Value(perimeter))
  else:
      print("No solution found.")
  </code></pre>
  
      
    <pre><code># Alternate version: Compute total land perimeter using CSP (no BFS)
        from ortools.sat.python import cp_model
        
        def compute_land_perimeter_csp(grid):
            model = cp_model.CpModel()
            rows, cols = len(grid), len(grid[0])
            perimeter_vars = []
        
            for i in range(rows):
                for j in range(cols):
                    if grid[i][j] == 1:  # Only consider land cells
                        for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:  # up, down, left, right
                            ni, nj = i + dx, j + dy
                            var = model.NewBoolVar(f"perim_{i}_{j}_{dx}_{dy}")
                            
                            if ni &lt; 0 or ni &gt;= rows or nj &lt; 0 or nj &gt;= cols:
                                # Neighbor is out-of-bounds: always contributes to perimeter
                                model.Add(var == 1)
                            else:
                                # Neighbor is water: contributes to perimeter
                                if grid[ni][nj] == 0:
                                    model.Add(var == 1)
                                else:
                                    model.Add(var == 0)
        
                            perimeter_vars.append(var)
        
            # Objective: compute total perimeter
            total_perimeter = model.NewIntVar(0, len(perimeter_vars), "total_perimeter")
            model.Add(total_perimeter == sum(perimeter_vars))
        
            # Solve
            solver = cp_model.CpSolver()
            status = solver.Solve(model)
        
            if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:
                print(f"Computed land perimeter using CSP: {solver.Value(total_perimeter)}")
            else:
                print("No solution found.")
        
        # Example grid
        grid = [
            [0, 1, 1, 0, 0],
            [0, 1, 1, 0, 0],
            [0, 0, 0, 0, 0],
            [1, 1, 0, 1, 1],
            [1, 1, 0, 1, 1],
        ]
        
        compute_land_perimeter_csp(grid)</code></pre>
        <h2>üß≥ TSP with Permutations (10 Cities)</h2>
        <pre><code>from ortools.sat.python import cp_model
        import random
        
        # Number of cities
        N = 10
        
        # Generate random distance matrix (symmetric, 0 on diagonal)
        random.seed(42)
        distance = [[0 if i == j else random.randint(10, 100) for j in range(N)] for i in range(N)]
        for i in range(N):
            for j in range(i + 1, N):
                distance[j][i] = distance[i][j]
        
        # Create the model
        model = cp_model.CpModel()
        
        # city[i] = index of the city visited at position i in the tour
        city = [model.NewIntVar(0, N - 1, f'city_{i}') for i in range(N)]
        
        # All cities must be visited exactly once
        model.AddAllDifferent(city)
        
        # Create variables for distances between consecutive cities
        cost_vars = []
        for i in range(N):
            next_i = (i + 1) % N  # Return to start after last city
            dist = model.NewIntVar(0, 100, f'dist_{i}')
            model.AddElement(
                model.NewConstant(i),
                [distance[i][j] for j in range(N)],
                dist
            )
            cost_vars.append(dist)
        
        # Total tour cost
        total_cost = model.NewIntVar(0, 10000, 'total_cost')
        model.Add(total_cost == sum(cost_vars))
        model.Minimize(total_cost)
        
        # Solve the model
        solver = cp_model.CpSolver()
        status = solver.Solve(model)
        
        # Print solution
        if status in (cp_model.OPTIMAL, cp_model.FEASIBLE):
            tour = [solver.Value(c) for c in city]
            print("Tour:", tour + [tour[0]])  # Include return to start
            print("Total cost:", solver.Value(total_cost))
        else:
            print("No solution found.")
        </code></pre>

        <h2>üëë N-Queens Problem (e.g., N = 8)</h2>
        <pre><code>from ortools.sat.python import cp_model
        
        def solve_n_queens(n):
            model = cp_model.CpModel()
        
            # Variables: queen_in_row[i] = column position of queen in row i
            queens = [model.NewIntVar(0, n - 1, f'Q_{i}') for i in range(n)]
        
            # Constraint 1: All queens must be in different columns
            model.AddAllDifferent(queens)
        
            # Constraint 2: No two queens on the same diagonal
            for i in range(n):
                for j in range(i + 1, n):
                    model.Add(queens[i] != queens[j])
                    model.Add(queens[i] - i != queens[j] - j)  # \ diagonal
                    model.Add(queens[i] + i != queens[j] + j)  # / diagonal
        
            # Solve
            solver = cp_model.CpSolver()
            status = solver.Solve(model)
        
            # Output solution
            if status in (cp_model.FEASIBLE, cp_model.OPTIMAL):
                print(f"\nSolution for {n}-Queens:")
                for i in range(n):
                    row = ['.'] * n
                    row[solver.Value(queens[i])] = 'Q'
                    print(' '.join(row))
            else:
                print("No solution found.")
        
        # Example: Solve for N = 8
        solve_n_queens(8)
        </code></pre>

        <h2>üìà Optimization Problem (Maximize Objective)</h2>
        <pre><code>from ortools.sat.python import cp_model
        
        model = cp_model.CpModel()
        
        x = model.new_int_var(0, 10, "x")
        y = model.new_int_var(0, 10, "y")
        z = model.new_int_var(0, 10, "z")
        
        model.add(2 * x + y + z <= 20)
        model.add(3 * x - 2 * y + z <= 25)
        
        model.maximize(2 * x + 3 * y + 5 * z)
        
        solver = cp_model.CpSolver()
        status = solver.solve(model)
        
        print(f"Objective = {solver.objective_value}")
        print(f"x = {solver.value(x)}, y = {solver.value(y)}, z = {solver.value(z)}")
        </code></pre>

        
        <h2>üîÅ Get All Solutions</h2>
<pre><code>from ortools.sat.python import cp_model

class SolutionPrinter(cp_model.CpSolverSolutionCallback):
    def __init__(self, vars):
        cp_model.CpSolverSolutionCallback.__init__(self)
        self.vars = vars

    def on_solution_callback(self):
        print("Solution:", [self.Value(v) for v in self.vars])

model = cp_model.CpModel()

x = model.new_int_var(0, 2, "x")
y = model.new_int_var(0, 2, "y")
z = model.new_int_var(0, 2, "z")

model.add(x != y)

solver = cp_model.CpSolver()
solver.parameters.enumerate_all_solutions = True
solver.solve(model, SolutionPrinter([x, y, z]))
</code></pre>

<h1>üìä Supervised Learning ‚Äì Linear, Logistic, Decision Tree, Gaussian Naive Bayes (ALL in ONE)</h1>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('Amazon_Sale_Report.csv')

# Drop duplicates if any
df.drop_duplicates(inplace=True)

# Identify and handle missing values
imputer = SimpleImputer(strategy='most_frequent')
df_cleaned = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Convert categorical variables using Label Encoding
label_encoders = {}
for col in df_cleaned.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df_cleaned[col] = le.fit_transform(df_cleaned[col])
    label_encoders[col] = le

# Separate features and target
X = df_cleaned.drop('B2B', axis=1)
y = df_cleaned['B2B']

# Standardize numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize models
lr_model = LinearRegression()
nb_model = GaussianNB()
log_model = LogisticRegression(max_iter=1000)
dt_model = DecisionTreeClassifier(random_state=42)

# Fit models
lr_model.fit(X_train, y_train)
nb_model.fit(X_train, y_train)
log_model.fit(X_train, y_train)
dt_model.fit(X_train, y_train)

# Linear regression outputs continuous values, convert to binary
lr_preds = lr_model.predict(X_test)
lr_preds_binary = np.where(lr_preds >= 0.5, 1, 0)

# Predictions
nb_preds = nb_model.predict(X_test)
log_preds = log_model.predict(X_test)
dt_preds = dt_model.predict(X_test)

# Evaluation on test set
print("\n--- Test Set Evaluation ---")
print("Logistic Regression:\n", classification_report(y_test, log_preds))
print("Naive Bayes:\n", classification_report(y_test, nb_preds))
print("Decision Tree:\n", classification_report(y_test, dt_preds))
print("Linear Regression:\n", classification_report(y_test, lr_preds_binary))

# Plot Confusion Matrices
models = {
    "Linear Regression": (lr_preds_binary, "Blues"),
    "Naive Bayes": (nb_preds, "Greens"),
    "Logistic Regression": (log_preds, "Oranges"),
    "Decision Tree": (dt_preds, "Purples")
}

for name, (preds, cmap) in models.items():
    cm = confusion_matrix(y_test, preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap=cmap)
    plt.title(f"{name} - Confusion Matrix")
    plt.show()

# K-Fold Cross Validation (5 folds)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
print("\n--- K-Fold Cross Validation (cv=5) ---")
for model, name in zip([log_model, nb_model, dt_model], ["Logistic Regression", "Naive Bayes", "Decision Tree"]):
    scores = cross_val_score(model, X_scaled, y, cv=kf)
    print(f"{name} Mean Accuracy: {scores.mean():.4f}, Scores: {scores}")</code></pre>



    <h1>üìâ Unsupervised Learning ‚Äì Customer Segmentation with K-Means</h1>
<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

# Load dataset
df = pd.read_csv("Customer_Segments.csv")

# Handle missing values
imputer = SimpleImputer(strategy='most_frequent')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Encode categorical columns
label_encoders = {}
for col in df_imputed.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df_imputed[col] = le.fit_transform(df_imputed[col])
    label_encoders[col] = le

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_imputed)

# Use Elbow Method to find optimal k
wcss = []
k_range = range(1, 11)
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

plt.plot(k_range, wcss, 'bo-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('WCSS')
plt.title('Elbow Method for Optimal k')
plt.grid(True)
plt.show()

# From the plot, assume optimal k = 3
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
cluster_labels = kmeans.fit_predict(X_scaled)

# Add cluster labels to original data
df['Cluster'] = cluster_labels

# Silhouette Score
score = silhouette_score(X_scaled, cluster_labels)
print(f"Silhouette Score for k={k}: {score:.4f}")

# Reduce to 2D for visualization using PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='Set1')
plt.title("Customer Segments using K-Means")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.grid(True)
plt.show()

# Optional: Describe clusters
print(df.groupby('Cluster').mean())</code></pre>

<h2>üìå K-Means with Elbow Method (Student Clustering)</h2>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import numpy as np

data = {}

df = pd.DataFrame(data)

features = df[["GPA", "study_hours", "attendance_rate"]]
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

wcss = []
for i in range(2, 7):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(scaled_features)
    wcss.append(kmeans.inertia_)

plt.plot(range(2, 7), wcss, marker="o")
plt.title("Elbow Method to Determine Optimal K")
plt.xlabel("Number of clusters (K)")
plt.ylabel("WCSS")
plt.grid(True)
plt.show()

optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans.fit_predict(scaled_features)
df["cluster"] = clusters

plt.figure(figsize=(8, 6))
plt.scatter(df["study_hours"], df["GPA"], c=clusters, cmap="Set1", s=100)
plt.title("Student Clusters Based on Study Hours and GPA")
plt.xlabel("Average Weekly Study Hours")
plt.ylabel("GPA")
plt.grid(True)
plt.show()

print(df[["student_id", "cluster"]])</code></pre>


<h2>üöó K-Means Clustering ‚Äì Vehicle Data (With vs Without Scaling)</h2>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
import numpy as np

data = {}
df = pd.DataFrame(data)

encoder = LabelEncoder()
df["vehicle_type_encoded"] = encoder.fit_transform(df["vehicle_type"])

features_original = df[
    ["mileage", "fuel_efficiency", "maintenance_cost", "vehicle_type_encoded"]
]

kmeans_no_scaling = KMeans(n_clusters=3, random_state=42)
clusters_no_scaling = kmeans_no_scaling.fit_predict(features_original)

pca = PCA(n_components=2)
pca_no_scaling = pca.fit_transform(features_original)

features_scaled = df[["mileage", "fuel_efficiency", "maintenance_cost"]].copy()
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_scaled)
features_scaled_combined = np.hstack(
    [features_scaled, df[["vehicle_type_encoded"]].values]
)

kmeans_with_scaling = KMeans(n_clusters=3, random_state=42)
clusters_with_scaling = kmeans_with_scaling.fit_predict(features_scaled_combined)

pca_scaled = pca.fit_transform(features_scaled_combined)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.scatter(
    pca_no_scaling[:, 0],
    pca_no_scaling[:, 1],
    c=clusters_no_scaling,
    cmap="Set1",
    s=100,
)
plt.title("KMeans Clustering (No Scaling)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")

plt.subplot(1, 2, 2)
plt.scatter(
    pca_scaled[:, 0], pca_scaled[:, 1], c=clusters_with_scaling, cmap="Set2", s=100
)
plt.title("KMeans Clustering (With Scaling)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")

plt.tight_layout()
plt.show()</code></pre>


<h2>üìÇ Logistic Regression with K-Fold Cross Validation</h2>
<pre><code>from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import pandas as pd
import seaborn as sns

# Load dataset
df = sns.load_dataset("titanic")

# Select features and target, handling missing values
X = df[["age", "fare"]].fillna(df[["age", "fare"]].mean())
y = df["survived"]

# Convert to DataFrame and Series to ensure .iloc[] compatibility
X = pd.DataFrame(X)
y = pd.Series(y)

# Define K-Fold (5 splits)
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Initialize model
model = LogisticRegression()

# Store accuracy scores
accuracy_scores = []

# Perform K-Fold Cross Validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Train model
    model.fit(X_train, y_train)

    # Predict and evaluate
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    accuracy_scores.append(acc)

# Print results
print("Accuracy scores across 5 folds:", accuracy_scores)
print(f"Average Accuracy: {sum(accuracy_scores)/len(accuracy_scores):.4f}")
</code></pre>


<h2>üìà Linear Regression ‚Äì Training, Prediction, and R¬≤ Evaluation</h2>
<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42
)

# Create and train the Linear Regression model
LR = LinearRegression()
ModelLR = LR.fit(x_train, y_train)

# Predict on the test data
PredictionLR = ModelLR.predict(x_test)

# Print the predictions
print("Predictions:", PredictionLR)

from sklearn.metrics import r2_score

print("===================LR Testing Accuracy================")
teachLR = r2_score(y_test, PredictionLR)
testingAccLR = teachLR * 100
print(testingAccLR)
</code></pre>


<h2>üîÅ Leave-One-Out Cross Validation (LOOCV)</h2>
<pre><code>from sklearn.model_selection import LeaveOneOut
import seaborn as sns
import pandas as pd

df = sns.load_dataset("titanic")
X = df[["age", "fare"]].fillna(df[["age", "fare"]].mean())
y = df["survived"]

X = pd.DataFrame(X)
y = pd.Series(y)

# Initialize LOOCV
loo = LeaveOneOut()

# Store accuracy scores
loo_scores = []

# Perform LOOCV
for train_index, test_index in loo.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # TODO: Train and evaluate a model here
    # e.g., fit LogisticRegression, make prediction, append accuracy
</code></pre>


<h2>üßº Supervised Learning ‚Äì Data Preprocessing</h2>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('Amazon_Sale_Report.csv')

# Drop duplicates if any
df.drop_duplicates(inplace=True)

# Identify missing values and fill with mean
missing_values = df.isnull().sum()
print("\nMissing values before imputation:\n", missing_values)

# Impute missing numeric values with the mean
imputer = SimpleImputer(strategy='mean')
df_cleaned = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# After imputation, check again for missing values
missing_values_after_imputation = df_cleaned.isnull().sum()
print("\nMissing values after imputation:\n", missing_values_after_imputation)

# Convert categorical variables using Label Encoding
label_encoders = {}
for col in df_cleaned.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df_cleaned[col] = le.fit_transform(df_cleaned[col])
    label_encoders[col] = le

# Separate features and target
X = df_cleaned.drop('B2B', axis=1)
y = df_cleaned['B2B']

# Standardize numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
</code></pre>


<h2>üß™ Supervised Learning ‚Äì Train-Test-Split, Initialize and Predict Models</h2>
<pre><code># Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize models
lr_model = LinearRegression()
nb_model = GaussianNB()
log_model = LogisticRegression(max_iter=1000)
dt_model = DecisionTreeClassifier(random_state=42)

# Fit models
lr_model.fit(X_train, y_train)
nb_model.fit(X_train, y_train)
log_model.fit(X_train, y_train)
dt_model.fit(X_train, y_train)

# Linear regression outputs continuous values, convert to binary
lr_preds = lr_model.predict(X_test)
lr_preds_binary = np.where(lr_preds >= 0.5, 1, 0)

# Predictions
nb_preds = nb_model.predict(X_test)
log_preds = log_model.predict(X_test)
dt_preds = dt_model.predict(X_test)
</code></pre>


<h2>üìä Supervised Learning ‚Äì Evaluate Models, Confusion Matrix, K-Fold</h2>
<pre><code># Evaluation on test set
print("\n--- Test Set Evaluation ---")
print("Logistic Regression:\n", classification_report(y_test, log_preds))
print("Naive Bayes:\n", classification_report(y_test, nb_preds))
print("Decision Tree:\n", classification_report(y_test, dt_preds))
print("Linear Regression:\n", classification_report(y_test, lr_preds_binary))

# Plot Confusion Matrices
models = {
    "Linear Regression": (lr_preds_binary, "Blues"),
    "Naive Bayes": (nb_preds, "Greens"),
    "Logistic Regression": (log_preds, "Oranges"),
    "Decision Tree": (dt_preds, "Purples")
}

for name, (preds, cmap) in models.items():
    cm = confusion_matrix(y_test, preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap=cmap)
    plt.title(f"{name} - Confusion Matrix")
    plt.show()

# K-Fold Cross Validation (5 folds)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
print("\n--- K-Fold Cross Validation (cv=5) ---")
for model, name in zip([log_model, nb_model, dt_model], ["Logistic Regression", "Naive Bayes", "Decision Tree"]):
    scores = cross_val_score(model, X_scaled, y, cv=kf)
    print(f"{name} Mean Accuracy: {scores.mean():.4f}, Scores: {scores}")
</code></pre>


</body>
</html>
