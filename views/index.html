<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Jupyter Notebook</title>
  <style>
    body { font-family: Arial, sans-serif; padding: 20px; background: #f4f4f4; }
    h2 { margin-top: 40px; color: #333; }
    pre { background: #fff; padding: 12px; border-left: 4px solid #007acc; overflow-x: auto; }
    code { font-family: Consolas, monospace; font-size: 14px; white-space: pre-wrap; }
  </style>
</head>
<body>

    <h1>üìö Table of Contents</h1>
<ul>
  <li><a href="#csp-basic">Basic CSP Template</a></li>
  <li><a href="#csp-seating">Seating Arrangement Puzzle</a></li>
  <li><a href="#csp-schedule">Schedule Tasks</a></li>
  <li><a href="#csp-robot">Diagonal Robot Navigation</a></li>
  <li><a href="#csp-island">Island Perimeter Detection</a></li>
  <li><a href="#csp-tsp">TSP with Permutations</a></li>
  <li><a href="#csp-nqueens">N-Queens Problem</a></li>
  <li><a href="#csp-optimization">Optimization Problem</a></li>
  <li><a href="#csp-allsolutions">Get All Solutions</a></li>
  <li><a href="#ml-supervised">Supervised Learning (All Models)</a></li>
  <li><a href="#ml-preprocessing">Data Preprocessing</a></li>
  <li><a href="#ml-train-predict">Train-Test-Split + Predict</a></li>
  <li><a href="#ml-evaluate">Model Evaluation + K-Fold</a></li>
  <li><a href="#ml-onehot">One Hot Encoding</a></li>
  <li><a href="#ml-roc">ROC Curve</a></li>
  <li><a href="#ml-outlier">Outlier Detection</a></li>
  <li><a href="#ml-confusion">Confusion Matrix</a></li>
  <li><a href="#ml-svm">Support Vector Machine</a></li>
  <li><a href="#ml-unsupervised">Unsupervised Learning (K-Means)</a></li>
</ul>
<hr>

  <h1>üß† Constraint Satisfaction Problem (CSP) Examples</h1>

  <h2 id="csp-basic">1. Basic CSP Template (Always Start With This)</h2>
<pre><code>
from ortools.sat.python import cp_model

model = cp_model.CpModel()

# Define variables
x = model.new_int_var(0, 2, "x")
y = model.new_int_var(0, 2, "y")

# Define constraints
model.add(x != y)

# Solve
solver = cp_model.CpSolver()
status = solver.solve(model)
if status in [cp_model.FEASIBLE, cp_model.OPTIMAL]:
    print(f"x = {solver.value(x)}, y = {solver.value(y)}")
else:
    print("No solution found.")
</code></pre>
  

  <h2 	id="csp-seating">ü™ë Seating Arrangement Puzzle (Logic Constraints)</h2>
<pre><code>
from ortools.sat.python import cp_model
  
model = cp_model.CpModel()

Amir = model.NewIntVar(1, 4, "Amir")
Bella = model.NewIntVar(0, 5, "Bella")
Charles = model.NewIntVar(0, 5, "Charles")
Diana = model.NewIntVar(1, 4, "Diana")
Ethan = model.NewIntVar(2, 3, "Ethan")
Farah = model.NewIntVar(0, 5, "Farah")

panelists = [Amir, Bella, Charles, Diana, Ethan, Farah]
names = ["Amir", "Bella", "Charles", "Diana", "Ethan", "Farah"]

model.AddAllDifferent(panelists)
model.Add(Bella < Farah)
model.AddAbsEquality(model.NewIntVar(1, 1, "Charles_Diana_adjacent"), Charles - Diana)

solver = cp_model.CpSolver()
status = solver.Solve(model)

if status == cp_model.FEASIBLE or status == cp_model.OPTIMAL:
    seat_map = {}
    for var, name in zip(panelists, names):
        seat_map[solver.Value(var)] = name

    print("Seat assignments (from left S0 to right S5):")
    for seat in range(6):
        print(f"Seat {seat}: {seat_map[seat]}")
else:
    print("No solution found.")
</code></pre>
  

<h2 id="csp-schedule">üìÜ Schedule Tasks ‚Äì College Course Timetable (No Conflicts)</h2>
<pre><code>
from ortools.sat.python import cp_model

# Step 1: Define data
courses = ['Math', 'Physics', 'Chemistry', 'CS']
conflicts = [('Math', 'Physics'), ('Physics', 'Chemistry'), ('Chemistry', 'CS')]
time_slots = range(3)  # 0 = Morning, 1 = Afternoon, 2 = Evening

# Step 2: Create model
model = cp_model.CpModel()

# Step 3: Create variables for each course (domain = 0, 1, 2)
course_vars = {
    course: model.NewIntVar(0, len(time_slots) - 1, course) for course in courses
}

# Step 4: Add constraints for conflicting courses
for c1, c2 in conflicts:
    model.Add(course_vars[c1] != course_vars[c2])

# Step 5: Solve the model
solver = cp_model.CpSolver()
status = solver.Solve(model)

# Step 6: Display the result
if status == cp_model.FEASIBLE or status == cp_model.OPTIMAL:
    print("Course Schedule:")
    slot_names = ['Morning', 'Afternoon', 'Evening']
    for course in courses:
        print(f"{course}: {slot_names[solver.Value(course_vars[course])]}")
else:
    print("No feasible schedule found.")
</code></pre>

  
  <h2 id="csp-robot">ü§ñ 4. Diagonal Robot Navigation (5x5 Grid)</h2>
<pre><code>
from ortools.sat.python import cp_model
import math

GRID_SIZE = 5
obstacles = set()
start = (1, 1)
target = (4, 4)

valid_moves = [(-1, -1), (-1, 0), (-1, 1),
                (0, -1),           (0, 1),
                (1, -1),  (1, 0),  (1, 1)]

class DiagonalPathSolutionPrinter(cp_model.CpSolverSolutionCallback):
    def __init__(self, variables, grid_size):
        super().__init__()
        self.variables = variables
        self.grid_size = grid_size
        self.solution_count = 0

    def on_solution_callback(self):
        self.solution_count += 1
        path = []
        for i in range(len(self.variables)):
            x = self.Value(self.variables[i][0])
            y = self.Value(self.variables[i][1])
            path.append((x, y))
        print(f"Solution {self.solution_count}: {path}")

def solve_diagonal_path():
    model = cp_model.CpModel()
    max_steps = 2 * GRID_SIZE

    steps = [(model.NewIntVar(0, GRID_SIZE - 1, f"x_{i}"),
            model.NewIntVar(0, GRID_SIZE - 1, f"y_{i}"))
            for i in range(max_steps)]

    model.Add(steps[0][0] == start[0])
    model.Add(steps[0][1] == start[1])
    model.Add(steps[-1][0] == target[0])
    model.Add(steps[-1][1] == target[1])

    for i in range(max_steps - 1):
        dx = model.NewIntVar(-1, 1, f"dx_{i}")
        dy = model.NewIntVar(-1, 1, f"dy_{i}")
        model.AddAllowedAssignments([dx, dy], valid_moves)
        model.Add(steps[i + 1][0] == steps[i][0] + dx)
        model.Add(steps[i + 1][1] == steps[i][1] + dy)
        for ox, oy in obstacles:
            model.AddForbiddenAssignments([steps[i + 1][0], steps[i + 1][1]], [(ox, oy)])

    cost = sum(math.sqrt(2) for _ in range(max_steps - 1))
    model.Minimize(int(cost * 100))

    solver = cp_model.CpSolver()
    solution_printer = DiagonalPathSolutionPrinter(steps, GRID_SIZE)
    solver.parameters.enumerate_all_solutions = True
    status = solver.Solve(model, solution_printer)

    if status in (cp_model.OPTIMAL, cp_model.FEASIBLE):
        print("Optimal path found!")
    else:
        print("No valid path found.")

solve_diagonal_path()
</code></pre>
  

  <h2 id="csp-island">üåä Island Perimeter Detection (Largest Landmass via BFS + CSP)</h2>
<pre><code>
from ortools.sat.python import cp_model
from collections import deque

grid = [[0, 1, 1, 0], [1, 1, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0]]
rows = len(grid)
cols = len(grid[0])

def get_largest_landmass(grid):
    visited = [[False] * cols for _ in range(rows)]
    max_component = []

    def bfs(r, c):
        queue = deque()
        component = []
        queue.append((r, c))
        visited[r][c] = True

        while queue:
            x, y = queue.popleft()
            component.append((x, y))
            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
                nx, ny = x + dx, y + dy
                if 0 <= nx < rows and 0 <= ny < cols:
                    if not visited[nx][ny] and grid[nx][ny] == 1:
                        visited[nx][ny] = True
                        queue.append((nx, ny))
        return component

    for i in range(rows):
        for j in range(cols):
            if grid[i][j] == 1 and not visited[i][j]:
                comp = bfs(i, j)
                if len(comp) > len(max_component):
                    max_component = comp
    return max_component

largest_landmass = get_largest_landmass(grid)

model = cp_model.CpModel()
edge_vars = []

for r, c in largest_landmass:
    for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
        nr, nc = r + dr, c + dc
        is_edge = model.NewBoolVar(f"edge_{r}_{c}_{dr}_{dc}")
        if not (0 <= nr < rows and 0 <= nc < cols) or grid[nr][nc] == 0:
            model.Add(is_edge == 1)
        else:
            model.Add(is_edge == 0)
        edge_vars.append(is_edge)

perimeter = model.NewIntVar(0, len(edge_vars), "perimeter")
model.Add(perimeter == sum(edge_vars))
model.Maximize(perimeter)

solver = cp_model.CpSolver()
status = solver.Solve(model)

if status in (cp_model.OPTIMAL, cp_model.FEASIBLE):
    print("Perimeter of the largest landmass:", solver.Value(perimeter))
else:
    print("No solution found.")
</code></pre>
  
<pre><code># Alternate version: Compute total land perimeter using CSP (no BFS)
from ortools.sat.python import cp_model

def compute_land_perimeter_csp(grid):
    model = cp_model.CpModel()
    rows, cols = len(grid), len(grid[0])
    perimeter_vars = []

    for i in range(rows):
        for j in range(cols):
            if grid[i][j] == 1:  # Only consider land cells
                for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:  # up, down, left, right
                    ni, nj = i + dx, j + dy
                    var = model.NewBoolVar(f"perim_{i}_{j}_{dx}_{dy}")
                    
                    if ni &lt; 0 or ni &gt;= rows or nj &lt; 0 or nj &gt;= cols:
                        # Neighbor is out-of-bounds: always contributes to perimeter
                        model.Add(var == 1)
                    else:
                        # Neighbor is water: contributes to perimeter
                        if grid[ni][nj] == 0:
                            model.Add(var == 1)
                        else:
                            model.Add(var == 0)

                    perimeter_vars.append(var)

    # Objective: compute total perimeter
    total_perimeter = model.NewIntVar(0, len(perimeter_vars), "total_perimeter")
    model.Add(total_perimeter == sum(perimeter_vars))

    # Solve
    solver = cp_model.CpSolver()
    status = solver.Solve(model)

    if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:
        print(f"Computed land perimeter using CSP: {solver.Value(total_perimeter)}")
    else:
        print("No solution found.")

# Example grid
grid = [
    [0, 1, 1, 0, 0],
    [0, 1, 1, 0, 0],
    [0, 0, 0, 0, 0],
    [1, 1, 0, 1, 1],
    [1, 1, 0, 1, 1],
]

compute_land_perimeter_csp(grid)
</code></pre>

        <h2 id="csp-tsp">üß≥ TSP with Permutations (10 Cities)</h2>
<pre><code>
from ortools.sat.python import cp_model
import random

# Number of cities
N = 10

# Generate random distance matrix (symmetric, 0 on diagonal)
random.seed(42)
distance = [[0 if i == j else random.randint(10, 100) for j in range(N)] for i in range(N)]
for i in range(N):
    for j in range(i + 1, N):
        distance[j][i] = distance[i][j]

# Create the model
model = cp_model.CpModel()

# city[i] = index of the city visited at position i in the tour
city = [model.NewIntVar(0, N - 1, f'city_{i}') for i in range(N)]

# All cities must be visited exactly once
model.AddAllDifferent(city)

# Create variables for distances between consecutive cities
cost_vars = []
for i in range(N):
    next_i = (i + 1) % N  # Return to start after last city
    dist = model.NewIntVar(0, 100, f'dist_{i}')
    model.AddElement(
        model.NewConstant(i),
        [distance[i][j] for j in range(N)],
        dist
    )
    cost_vars.append(dist)

# Total tour cost
total_cost = model.NewIntVar(0, 10000, 'total_cost')
model.Add(total_cost == sum(cost_vars))
model.Minimize(total_cost)

# Solve the model
solver = cp_model.CpSolver()
status = solver.Solve(model)

# Print solution
if status in (cp_model.OPTIMAL, cp_model.FEASIBLE):
    tour = [solver.Value(c) for c in city]
    print("Tour:", tour + [tour[0]])  # Include return to start
    print("Total cost:", solver.Value(total_cost))
else:
    print("No solution found.")
</code></pre>

        <h2 id="csp-nqueens">üëë N-Queens Problem (e.g., N = 8)</h2>
<pre><code>
from ortools.sat.python import cp_model

def solve_n_queens(n):
    model = cp_model.CpModel()

    # Variables: queen_in_row[i] = column position of queen in row i
    queens = [model.NewIntVar(0, n - 1, f'Q_{i}') for i in range(n)]

    # Constraint 1: All queens must be in different columns
    model.AddAllDifferent(queens)

    # Constraint 2: No two queens on the same diagonal
    for i in range(n):
        for j in range(i + 1, n):
            model.Add(queens[i] != queens[j])
            model.Add(queens[i] - i != queens[j] - j)  # \ diagonal
            model.Add(queens[i] + i != queens[j] + j)  # / diagonal

    # Solve
    solver = cp_model.CpSolver()
    status = solver.Solve(model)

    # Output solution
    if status in (cp_model.FEASIBLE, cp_model.OPTIMAL):
        print(f"\nSolution for {n}-Queens:")
        for i in range(n):
            row = ['.'] * n
            row[solver.Value(queens[i])] = 'Q'
            print(' '.join(row))
    else:
        print("No solution found.")

# Example: Solve for N = 8
solve_n_queens(8)
</code></pre>


<h2 	id="csp-optimization">üìà Optimization Problem (Maximize Objective)</h2>
<pre><code>
from ortools.sat.python import cp_model

model = cp_model.CpModel()

x = model.new_int_var(0, 10, "x")
y = model.new_int_var(0, 10, "y")
z = model.new_int_var(0, 10, "z")

model.add(2 * x + y + z <= 20)
model.add(3 * x - 2 * y + z <= 25)

model.maximize(2 * x + 3 * y + 5 * z)

solver = cp_model.CpSolver()
status = solver.solve(model)

print(f"Objective = {solver.objective_value}")
print(f"x = {solver.value(x)}, y = {solver.value(y)}, z = {solver.value(z)}")
</code></pre>

        
        <h2 id="csp-allsolutions">üîÅ Get All Solutions</h2>
<pre><code>
from ortools.sat.python import cp_model

class SolutionPrinter(cp_model.CpSolverSolutionCallback):
    def __init__(self, vars):
        cp_model.CpSolverSolutionCallback.__init__(self)
        self.vars = vars

    def on_solution_callback(self):
        print("Solution:", [self.Value(v) for v in self.vars])

model = cp_model.CpModel()

x = model.new_int_var(0, 2, "x")
y = model.new_int_var(0, 2, "y")
z = model.new_int_var(0, 2, "z")

model.add(x != y)

solver = cp_model.CpSolver()
solver.parameters.enumerate_all_solutions = True
solver.solve(model, SolutionPrinter([x, y, z]))
</code></pre>

    <h1 id="ml-unsupervised">üìâ Unsupervised Learning ‚Äì Customer Segmentation with K-Means</h1>
<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

# Load dataset
df = pd.read_csv("Customer_Segments.csv")

# Handle missing values
imputer = SimpleImputer(strategy='most_frequent')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Encode categorical columns
label_encoders = {}
for col in df_imputed.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df_imputed[col] = le.fit_transform(df_imputed[col])
    label_encoders[col] = le

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_imputed)

# Use Elbow Method to find optimal k
wcss = []
k_range = range(1, 11)
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

plt.plot(k_range, wcss, 'bo-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('WCSS')
plt.title('Elbow Method for Optimal k')
plt.grid(True)
plt.show()

# From the plot, assume optimal k = 3
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
cluster_labels = kmeans.fit_predict(X_scaled)

# Add cluster labels to original data
df['Cluster'] = cluster_labels

# Silhouette Score
score = silhouette_score(X_scaled, cluster_labels)
print(f"Silhouette Score for k={k}: {score:.4f}")

# Reduce to 2D for visualization using PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='Set1')
plt.title("Customer Segments using K-Means")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.grid(True)
plt.show()

# Optional: Describe clusters
print(df.groupby('Cluster').mean())</code></pre>


<h2>üöó K-Means Clustering ‚Äì Vehicle Data (With vs Without Scaling)</h2>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
import numpy as np

data = {}
df = pd.DataFrame(data)

encoder = LabelEncoder()
df["vehicle_type_encoded"] = encoder.fit_transform(df["vehicle_type"])

features_original = df[
    ["mileage", "fuel_efficiency", "maintenance_cost", "vehicle_type_encoded"]
]

kmeans_no_scaling = KMeans(n_clusters=3, random_state=42)
clusters_no_scaling = kmeans_no_scaling.fit_predict(features_original)

pca = PCA(n_components=2)
pca_no_scaling = pca.fit_transform(features_original)

features_scaled = df[["mileage", "fuel_efficiency", "maintenance_cost"]].copy()
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_scaled)
features_scaled_combined = np.hstack(
    [features_scaled, df[["vehicle_type_encoded"]].values]
)

kmeans_with_scaling = KMeans(n_clusters=3, random_state=42)
clusters_with_scaling = kmeans_with_scaling.fit_predict(features_scaled_combined)

pca_scaled = pca.fit_transform(features_scaled_combined)

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.scatter(
    pca_no_scaling[:, 0],
    pca_no_scaling[:, 1],
    c=clusters_no_scaling,
    cmap="Set1",
    s=100,
)
plt.title("KMeans Clustering (No Scaling)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")

plt.subplot(1, 2, 2)
plt.scatter(
    pca_scaled[:, 0], pca_scaled[:, 1], c=clusters_with_scaling, cmap="Set2", s=100
)
plt.title("KMeans Clustering (With Scaling)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")

plt.tight_layout()
plt.show()</code></pre>


<h2 id="ml-supervised">üìÇ Logistic Regression with K-Fold Cross Validation</h2>
<pre><code>from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import pandas as pd
import seaborn as sns

# Load dataset
df = sns.load_dataset("titanic")

# Select features and target, handling missing values
X = df[["age", "fare"]].fillna(df[["age", "fare"]].mean())
y = df["survived"]

# Convert to DataFrame and Series to ensure .iloc[] compatibility
X = pd.DataFrame(X)
y = pd.Series(y)

# Define K-Fold (5 splits)
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Initialize model
model = LogisticRegression()

# Store accuracy scores
accuracy_scores = []

# Perform K-Fold Cross Validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Train model
    model.fit(X_train, y_train)

    # Predict and evaluate
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    accuracy_scores.append(acc)

# Print results
print("Accuracy scores across 5 folds:", accuracy_scores)
print(f"Average Accuracy: {sum(accuracy_scores)/len(accuracy_scores):.4f}")
</code></pre>


<h2>üìà Linear Regression ‚Äì Training, Prediction, and R¬≤ Evaluation</h2>
<pre><code>import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42
)

# Create and train the Linear Regression model
LR = LinearRegression()
ModelLR = LR.fit(x_train, y_train)

# Predict on the test data
PredictionLR = ModelLR.predict(x_test)

# Print the predictions
print("Predictions:", PredictionLR)

from sklearn.metrics import r2_score

print("===================LR Testing Accuracy================")
teachLR = r2_score(y_test, PredictionLR)
testingAccLR = teachLR * 100
print(testingAccLR)
</code></pre>


<h2>üîÅ Leave-One-Out Cross Validation (LOOCV)</h2>
<pre><code>from sklearn.model_selection import LeaveOneOut
import seaborn as sns
import pandas as pd

df = sns.load_dataset("titanic")
X = df[["age", "fare"]].fillna(df[["age", "fare"]].mean())
y = df["survived"]

X = pd.DataFrame(X)
y = pd.Series(y)

# Initialize LOOCV
loo = LeaveOneOut()

# Store accuracy scores
loo_scores = []

# Perform LOOCV
for train_index, test_index in loo.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # TODO: Train and evaluate a model here
    # e.g., fit LogisticRegression, make prediction, append accuracy
</code></pre>


<h2 id="ml-preprocessing">üßº Supervised Learning ‚Äì Data Preprocessing</h2>
<pre><code>
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

# Load dataset
df = pd.read_csv('Amazon_Sale_Report.csv')

# Drop duplicates if any
df.drop_duplicates(inplace=True)

# Identify missing values and fill with mean
missing_values = df.isnull().sum()
print("\nMissing values before imputation:\n", missing_values)

# Impute missing numeric values with the mean
imputer = SimpleImputer(strategy='mean')
df_cleaned = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# After imputation, check again for missing values
missing_values_after_imputation = df_cleaned.isnull().sum()
print("\nMissing values after imputation:\n", missing_values_after_imputation)

# Convert categorical variables using Label Encoding
label_encoders = {}
for col in df_cleaned.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df_cleaned[col] = le.fit_transform(df_cleaned[col])
    label_encoders[col] = le

# --- Correlation Heatmap ---
plt.figure(figsize=(12, 8))
corr_matrix = df_cleaned.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap")
plt.tight_layout()
plt.show()
# ----------------------------

# Separate features and target
X = df_cleaned.drop('B2B', axis=1)
y = df_cleaned['B2B']

# Standardize numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
</code></pre>
    
</code></pre>


<h2>üß™ Preprocessing Version 2 ‚Äì DATA PREPRCOESSING- Encoding, Scaling, Outliers, Correlation</h2>
<pre><code># LABEL ENCODING (useful for ordinal variables like low, medium, high)
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
data['name_encoded'] = label_encoder.fit_transform(data[name])

# ONE-HOT ENCODING (useful for nominal categorical data)
data_encoded = pd.get_dummies(data, columns=[name], drop_first=False)

# SCALING
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data['Age_standardized'] = scaler.fit_transform(data[['Age']])

# OUTLIER DETECTION USING IQR
Q1 = data['Age'].quantile(0.25)
Q3 = data['Age'].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = data[(data['Age'] < lower_bound) | (data['Age'] > upper_bound)]
print("Outliers detected using IQR method:\n", outliers)

# REMOVE OUTLIERS
data_cleaned = data[(data['Age'] >= lower_bound) & (data['Age'] <= upper_bound)]

# CORRELATION ANALYSIS
correlation_matrix = data.corr()
print(correlation_matrix)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()
</code></pre>


<h2 id="ml-train-predict">üß™ Supervised Learning ‚Äì Train-Test-Split, Initialize and Predict Models</h2>
<pre><code># Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize models
lr_model = LinearRegression()
nb_model = GaussianNB()
log_model = LogisticRegression(max_iter=1000)
dt_model = DecisionTreeClassifier(random_state=42)

# Fit models
lr_model.fit(X_train, y_train)
nb_model.fit(X_train, y_train)
log_model.fit(X_train, y_train)
dt_model.fit(X_train, y_train)

# Linear regression outputs continuous values, convert to binary
lr_preds = lr_model.predict(X_test)
lr_preds_binary = np.where(lr_preds >= 0.5, 1, 0)

# Predictions
nb_preds = nb_model.predict(X_test)
log_preds = log_model.predict(X_test)
dt_preds = dt_model.predict(X_test)
</code></pre>


<h2 id="ml-evaluate">üìä Supervised Learning ‚Äì Evaluate Models, Confusion Matrix, K-Fold</h2>
<pre><code># Evaluation on test set
print("\n--- Test Set Evaluation ---")
print("Logistic Regression:\n", classification_report(y_test, log_preds))
print("Naive Bayes:\n", classification_report(y_test, nb_preds))
print("Decision Tree:\n", classification_report(y_test, dt_preds))
print("Linear Regression:\n", classification_report(y_test, lr_preds_binary))

# Plot Confusion Matrices
models = {
    "Linear Regression": (lr_preds_binary, "Blues"),
    "Naive Bayes": (nb_preds, "Greens"),
    "Logistic Regression": (log_preds, "Oranges"),
    "Decision Tree": (dt_preds, "Purples")
}

for name, (preds, cmap) in models.items():
    cm = confusion_matrix(y_test, preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap=cmap)
    plt.title(f"{name} - Confusion Matrix")
    plt.show()

# K-Fold Cross Validation (5 folds)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
print("\n--- K-Fold Cross Validation (cv=5) ---")
for model, name in zip([log_model, nb_model, dt_model], ["Logistic Regression", "Naive Bayes", "Decision Tree"]):
    scores = cross_val_score(model, X_scaled, y, cv=kf)
    print(f"{name} Mean Accuracy: {scores.mean():.4f}, Scores: {scores}")
</code></pre>


<h2 	id="ml-onehot">üßÆ One Hot Encoding</h2>
<pre><code>
df = pd.get_dummies(df, columns=["embarked"], drop_first=True)
print(df.head())
</code></pre>

<h2 	id="ml-roc">üìâ ROC Curve</h2>
<pre><code>
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# After you get y_test and y_pred, also get the predicted probabilities
y_probs = model.predict_proba(X_test)[:, 1]  # Probability for class 1

# Compute FPR, TPR, thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_probs)

# Compute AUC
roc_auc = auc(fpr, tpr)

# Plot ROC Curve
plt.figure()
plt.plot(fpr, tpr, color="blue", label=f"ROC curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], color="gray", linestyle="--")  # Diagonal
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.grid()
plt.show()
</code></pre>


<h2 	id="ml-outlier">üìå Outlier Detection (IQR Method)</h2>
<pre><code>import pandas as pd

def remove_outliers_iqr(df, factor=1.5):
    """
    Removes rows with outliers in any numeric column based on the IQR method.
    
    Parameters:
        df (pd.DataFrame): Input dataframe.
        factor (float): Multiplier for IQR, default is 1.5.
        
    Returns:
        pd.DataFrame: DataFrame with outliers removed.
    """
    numeric_cols = df.select_dtypes(include=["number"]).columns
    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - factor * IQR
        upper_bound = Q3 + factor * IQR
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df
</code></pre>


<h2 	id="ml-confusion">üßæ Confusion Matrix</h2>
<pre><code>
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_iris

# Load sample data
X, y = load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Train model
model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)
</code></pre>



<h2 	id="ml-svm">üìê Support Vector Machine (SVM)</h2>
<pre><code>
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Binary classification: Classify class 0 vs others
y = (y == 0).astype(int)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Create and train SVM model
svm = SVC(kernel="rbf", C=1, gamma="scale")
svm.fit(X_train, y_train)

# Predict and evaluate
y_pred = svm.predict(X_test)
print("SVM Accuracy:", accuracy_score(y_test, y_pred))
</code></pre>


<h1 >üìä Supervised Learning ‚Äì Linear, Logistic, Decision Tree, Gaussian Naive Bayes (ALL in ONE)</h1>
<pre><code>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('Amazon_Sale_Report.csv')

# Drop duplicates if any
df.drop_duplicates(inplace=True)

# Identify and handle missing values
imputer = SimpleImputer(strategy='most_frequent')
df_cleaned = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Convert categorical variables using Label Encoding
label_encoders = {}
for col in df_cleaned.select_dtypes(include='object').columns:
    le = LabelEncoder()
    df_cleaned[col] = le.fit_transform(df_cleaned[col])
    label_encoders[col] = le

# Separate features and target
X = df_cleaned.drop('B2B', axis=1)
y = df_cleaned['B2B']

# Standardize numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize models
lr_model = LinearRegression()
nb_model = GaussianNB()
log_model = LogisticRegression(max_iter=1000)
dt_model = DecisionTreeClassifier(random_state=42)

# Fit models
lr_model.fit(X_train, y_train)
nb_model.fit(X_train, y_train)
log_model.fit(X_train, y_train)
dt_model.fit(X_train, y_train)

# Linear regression outputs continuous values, convert to binary
lr_preds = lr_model.predict(X_test)
lr_preds_binary = np.where(lr_preds >= 0.5, 1, 0)

# Predictions
nb_preds = nb_model.predict(X_test)
log_preds = log_model.predict(X_test)
dt_preds = dt_model.predict(X_test)

# Evaluation on test set
print("\n--- Test Set Evaluation ---")
print("Logistic Regression:\n", classification_report(y_test, log_preds))
print("Naive Bayes:\n", classification_report(y_test, nb_preds))
print("Decision Tree:\n", classification_report(y_test, dt_preds))
print("Linear Regression:\n", classification_report(y_test, lr_preds_binary))

# Plot Confusion Matrices
models = {
    "Linear Regression": (lr_preds_binary, "Blues"),
    "Naive Bayes": (nb_preds, "Greens"),
    "Logistic Regression": (log_preds, "Oranges"),
    "Decision Tree": (dt_preds, "Purples")
}

for name, (preds, cmap) in models.items():
    cm = confusion_matrix(y_test, preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap=cmap)
    plt.title(f"{name} - Confusion Matrix")
    plt.show()

# K-Fold Cross Validation (5 folds)
kf = KFold(n_splits=5, shuffle=True, random_state=42)
print("\n--- K-Fold Cross Validation (cv=5) ---")
for model, name in zip([log_model, nb_model, dt_model], ["Logistic Regression", "Naive Bayes", "Decision Tree"]):
    scores = cross_val_score(model, X_scaled, y, cv=kf)
    print(f"{name} Mean Accuracy: {scores.mean():.4f}, Scores: {scores}")</code></pre>



<h2>üßº Data Preprocessing ‚Äì Label Encoding and Missing Value Handling</h2>
<pre><code># Converts categories into numbers (e.g., male ‚Üí 0, female ‚Üí 1)
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['gender'] = le.fit_transform(df['gender'])  # 0 for male, 1 for female
print(df['gender'].head())

# Check for missing values
print(df.isnull().sum())

# Drop missing values
df_cleaned = df.dropna()
print("Dataset after dropping missing values:", df_cleaned.shape)

# Handle missing values
df['age'] = df['age'].fillna(df['age'].mean())  # Fill age with mean
df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])  # Fill embarked with mode

# Convert 'deck' to string and replace NaN
df['deck'] = df['deck'].astype(str).fillna('Unknown')

# Drop any remaining NaN values
df = df.dropna()
print(df.isnull().sum())  # Confirm no missing values
</code></pre>
   
<h2>üå≥ Binary Classifier ‚Äì Decision Tree on Iris Dataset</h2>
<pre><code># Step 0: Import libraries
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Step 1: Load dataset (Iris dataset as an example)
data = load_iris()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target

# Step 2: Split the data
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3‚Äì5: Train the decision tree model
clf = DecisionTreeClassifier(criterion="gini", max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))

# Plot the tree
plt.figure(figsize=(12, 8))
plot_tree(clf, feature_names=data.feature_names, class_names=data.target_names, filled=True)
plt.title("Decision Tree Visualization")
plt.show()
</code></pre>


</body>
</html>
